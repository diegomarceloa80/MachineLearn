---
title: K-Nearest Neighbors (KNN)
code-fold: false
---
" K-Nearest Neighbors (KNN) es un algoritmo de clasificación que se basa en la proximidad de los datos en el espacio de características. A continuación, se presenta una implementación básica de KNN utilizando scikit-learn. "
# Importar librerías

```{python}
import pandas as pd
from sklearn.datasets import fetch_openml
from sklearn.model_selection import GridSearchCV, cross_val_score # cross_val_score para validación cruzada permite evaluar el rendimiento k veces
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline # Pipeline para encadenar pasos de preprocesamiento y modelado
from sklearn.neighbors import KNeighborsClassifier # KNN 
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay # report y matriz de confusión
from sklearn.model_selection import train_test_split # train_test_split para dividir el dataset en conjunto de entrenamiento y prueba
```
# Cargar dataset
```{python}
# data set titanic

df = fetch_openml('titanic', version=1, as_frame=True)
df = df['frame']
df
```
# EDA información del dataset
```{python} 
df.info()   
```
# filtrado de columnas relevantes 
```{python}
df = df[['pclass','survived','sex','age','sibsp','parch','embarked']]
df
```
# eliminacion de datos faltantes y conversion del target

```{python}
df = df[df['survived'].notna()]
df['survived'] = df['survived'].astype(int)
```
# feactures y target
```{python}
X = df.drop( columns = 'survived', axis=1)
y = df['survived']
```
# Identificacion de feactures categoricas y numericas
```{python}
cat_cols = ['sex','embarked'] # categorical nominal
num_cols = ['pclass','age','sibsp','parch']
```
# Pipeline de preprocesamiento
## Generación de pipelines para datos categóricos y numéricos
```{python}
categorial_pipeline = Pipeline([
    ('imputer_cat', SimpleImputer(strategy='most_frequent')), # imputar valores faltantes con la moda
    ('onehot_cat', OneHotEncoder(handle_unknown='ignore')) # codificación one-hot para variables categóricas
])
numerical_pipeline = Pipeline([
    ('imputer_num', SimpleImputer(strategy='mean')), # imputar valores faltantes con la media
    ('scaler', StandardScaler()) # estandarización de características numéricas
])  

```
## Aplicar ColumnTransformer para combinar ambos pipelines
```{python}
preprocessor = ColumnTransformer(
        [('cat', categorial_pipeline, cat_cols),
         ('num', numerical_pipeline, num_cols)]
)
```
# Pileline
```{python}
pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('Clasificador', KNeighborsClassifier())
])  
```
# train test split
```{python}
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)
```
# grid search para hiperparámetros

## definir el param_grid

```{python}
param_grid = {
    'Clasificador__n_neighbors': list(range(1, 51)), # número de vecinos
    'Clasificador__weights': ['uniform', 'distance'], # tipo de ponderación
    'Clasificador__metric': ['minkowski','euclidean', 'manhattan'] # métrica de distancia
    
}
```
```{python}
grid = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')
grid.fit(X_train, y_train)
```
## Obtener el mejor best estimator
```{python}
best_model = grid.best_estimator_
```

```{python}
print("Mejores hiperparámetros:", grid.best_params_)
print("Mejor precisión en validación cruzada:", grid.best_score_)
```
## Evaluar el modelo en el conjunto de prueba
```{python}
y_pred = best_model.predict(X_test)
```
## Reporte de clasificación
```{python}
print('Primer reporte de clasificación\n',classification_report(y_test, y_pred))
print('Matriz de confusión\n',ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred)).plot())
```
